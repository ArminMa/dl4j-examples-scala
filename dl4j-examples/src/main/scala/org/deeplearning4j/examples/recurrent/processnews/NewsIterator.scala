/**
  * This is a DataSetIterator that is specialized for the News headlines dataset used in the TrainNews example
  * It takes either the train or test set data from this data set, plus a WordVectors object generated by
  * PrepareWordVector.java program and generates training data sets.<br>
  * Inputs/features: variable-length time series, where each word (with unknown words removed) is represented by
  * its Word2Vec vector representation.<br>
  * Labels/target: a single class (representing category, i.e. 0,1,2 etc. depending on content of categories.txt
  * file mentioned in TrainNews.java program.
  * <p>
  * Note :
  * - This program is a modification of original example named SentimentExampleIterator.java
  * - more details is given with each function's comments in the code
  * <p>
  * <b>KIT Solutions Pvt. Ltd. (www.kitsol.com)</b>
  *//**
  * This is a DataSetIterator that is specialized for the News headlines dataset used in the TrainNews example
  * It takes either the train or test set data from this data set, plus a WordVectors object generated by
  * PrepareWordVector.java program and generates training data sets.<br>
  * Inputs/features: variable-length time series, where each word (with unknown words removed) is represented by
  * its Word2Vec vector representation.<br>
  * Labels/target: a single class (representing category, i.e. 0,1,2 etc. depending on content of categories.txt
  * file mentioned in TrainNews.java program.
  * <p>
  * Note :
  * - This program is a modification of original example named SentimentExampleIterator.java
  * - more details is given with each function's comments in the code
  * <p>
  * <b>KIT Solutions Pvt. Ltd. (www.kitsol.com)</b>
  */
package org.deeplearning4j.examples.recurrent.processnews

import org.apache.commons.io.FileUtils
import org.apache.commons.lang3.tuple.Pair
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors
import org.deeplearning4j.text.tokenization.tokenizerfactory.TokenizerFactory
import org.nd4j.linalg.api.ndarray.INDArray
import org.nd4j.linalg.dataset.DataSet
import org.nd4j.linalg.dataset.api.DataSetPreProcessor
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator
import org.nd4j.linalg.factory.Nd4j
import org.nd4j.linalg.indexing.INDArrayIndex
import org.nd4j.linalg.indexing.NDArrayIndex._

import java.io.{BufferedReader, File, FileReader, IOException}
import java.util
import java.util.NoSuchElementException

import scala.collection.JavaConverters._

object NewsIterator {

  def builder = new NewsIterator.Builder

  class Builder private[processnews]() {
    private var dataDirectory: String = null
    private var wordVectors: WordVectors = null
    private var batchSize: Int = 0
    private var truncateLength: Int = 0
    private[processnews] var tokenizerFactory: TokenizerFactory = null
    private var train: Boolean = false

    def dataDirectory(dataDirectory: String): NewsIterator.Builder = {
      this.dataDirectory = dataDirectory
      this
    }

    def wordVectors(wordVectors: WordVectors): NewsIterator.Builder = {
      this.wordVectors = wordVectors
      this
    }

    def batchSize(batchSize: Int): NewsIterator.Builder = {
      this.batchSize = batchSize
      this
    }

    def truncateLength(truncateLength: Int): NewsIterator.Builder = {
      this.truncateLength = truncateLength
      this
    }

    def train(train: Boolean): NewsIterator.Builder = {
      this.train = train
      this
    }

    def tokenizerFactory(tokenizerFactory: TokenizerFactory): NewsIterator.Builder = {
      this.tokenizerFactory = tokenizerFactory
      this
    }

    def build: NewsIterator = {
      new NewsIterator(dataDirectory, wordVectors, batchSize, truncateLength, train, tokenizerFactory)
    }

    override def toString: String = {
      "org.deeplearning4j.examples.recurrent.ProcessNews.NewsIterator.Builder(dataDirectory=" + this.dataDirectory + ", wordVectors=" + this.wordVectors + ", batchSize=" + this.batchSize + ", truncateLength=" + this.truncateLength + ", train=" + this.train + ")"
    }
  }

}

/**
  * @param dataDirectory  the directory of the news headlines data set
  * @param wordVectors    WordVectors object
  * @param batchSize      Size of each minibatch for training
  * @param truncateLength If headline length exceed this size, it will be truncated to this size.
  * @param train          If true: return the training data. If false: return the testing data.
  *                       <p>
  *                       - initialize various class variables
  *                       - calls populateData function to load news data in categoryData vector
  *                       - also populates labels (i.e. category related inforamtion) in labels class variable
  */
class NewsIterator private(val dataDirectory: String, val wordVectors: WordVectors, val batchSize: Int, val truncateLength: Int, val train: Boolean, val tokenizerFactory: TokenizerFactory) extends DataSetIterator {

  private var maxLength: Int = 0
  private var _cursor: Int = 0
  private var totalNews: Int = 0
  private var newsPosition: Int = 0
  private var currCategory: Int = 0

  final private val vectorSize = wordVectors.getWordVector(wordVectors.vocab.wordAtIndex(0)).length
  final private val categoryData = new util.ArrayList[Pair[String, util.List[String]]]

  this.populateData(train)

  final private var labels = categoryData.asScala.map(_.getKey.split(",")(1)).toList.asJava



  def next(num: Int): DataSet = {
    if (_cursor >= this.totalNews) throw new NoSuchElementException
    try {
      nextDataSet(num)
    } catch { case e: IOException =>
      throw new RuntimeException(e)
    }
  }

  @throws[IOException]
  private def nextDataSet(num: Int): DataSet = {
    // Loads news into news list from categoryData List along with category of each news
    val news = new util.ArrayList[String](num)
    val category = new Array[Int](num)
    var k = 0
    while (k < num && _cursor < totalExamples) {
      if (currCategory < categoryData.size) {
        news.add(categoryData.get(currCategory).getValue.get(newsPosition))
        category(k) = categoryData.get(currCategory).getKey.split(",")(0).toInt
        currCategory += 1
        _cursor += 1
      } else {
        currCategory = 0
        newsPosition += 1
      }
      k += 1
    }

    //Second: tokenize news and filter out unknown words
    val allTokens = new util.ArrayList[util.List[String]](news.size)
    maxLength = 0
    for (s <- news.asScala) {
      val tokens = tokenizerFactory.create(s).getTokens
      val tokensFiltered = new util.ArrayList[String]
      for (t <- tokens.asScala) {
        if (wordVectors.hasWord(t)) tokensFiltered.add(t)
      }
      allTokens.add(tokensFiltered)
      maxLength = Math.max(maxLength, tokensFiltered.size)
    }

    //If longest news exceeds 'truncateLength': only take the first 'truncateLength' words
    //System.out.println("maxLength : " + maxLength);
    if (maxLength > truncateLength) maxLength = truncateLength

    //Create data for training
    //Here: we have news.size() examples of varying lengths
    val features = Nd4j.create(news.size, vectorSize, maxLength)
    val labels = Nd4j.create(news.size, this.categoryData.size, maxLength)
    //Three labels: Crime, Politics, Bollywood
    //Because we are dealing with news of different lengths and only one output at the final time step: use padding arrays
    //Mask arrays contain 1 if data is present at that time step for that example, or 0 if data is just padding
    val featuresMask = Nd4j.zeros(news.size, maxLength)
    val labelsMask = Nd4j.zeros(news.size, maxLength)

    val temp = new Array[Int](2)
    for (i <- news.asScala.indices) {
      val tokens = allTokens.get(i)
      temp(0) = i
      //Get word vectors for each word in news, and put them in the training data
      var j = 0
      for (j <- 0 until math.min(tokens.size, maxLength)) {
        val token = tokens.get(j)
        val vector = wordVectors.getWordVectorMatrix(token)
        features.put(Array[INDArrayIndex](point(i), all, point(j)), vector)

        temp(1) = j
        featuresMask.putScalar(temp, 1.0)
      }
      val idx = category(i)
      val lastIdx = Math.min(tokens.size, maxLength)
      labels.putScalar(Array[Int](i, idx, lastIdx - 1), 1.0)
      labelsMask.putScalar(Array[Int](i, lastIdx - 1), 1.0)
    }
    new DataSet(features, labels, featuresMask, labelsMask)
  }

  /**
    * Used post training to load a review from a file to a features INDArray that can be passed to the network output method
    *
    * @param file      File to load the review from
    * @param maxLength Maximum length (if review is longer than this: truncate to maxLength). Use Integer.MAX_VALUE to not nruncate
    * @return Features array
    * @throws IOException If file cannot be read
    */
  @throws[IOException]
  def loadFeaturesFromFile(file: File, maxLength: Int): INDArray = {
    val news = FileUtils.readFileToString(file)
    loadFeaturesFromString(news, maxLength)
  }

  /**
    * Used post training to convert a String to a features INDArray that can be passed to the network output method
    *
    * @param reviewContents Contents of the review to vectorize
    * @param maxLength      Maximum length (if review is longer than this: truncate to maxLength). Use Integer.MAX_VALUE to not nruncate
    * @return Features array for the given input String
    */
  def loadFeaturesFromString(reviewContents: String, maxLength: Int): INDArray = {
    val tokens = tokenizerFactory.create(reviewContents).getTokens
    val tokensFiltered = new util.ArrayList[String]
    for (t <- tokens.asScala) {
      if (wordVectors.hasWord(t)) tokensFiltered.add(t)
    }
    val outputLength = Math.max(maxLength, tokensFiltered.size)
    val features = Nd4j.create(1, vectorSize, outputLength)
    for (i <- 0 until math.min(tokens.size, maxLength)) {
      val token: String = tokens.get(i)
      val vector: INDArray = wordVectors.getWordVectorMatrix(token)
      features.put(Array[INDArrayIndex](point(0), all, point(i)), vector)
    }
    features
  }

  /*
   This function loads news headlines from files stored in resources into categoryData List.
   */
  private def populateData(train: Boolean) {
    val categories = new File(this.dataDirectory + File.separator + "categories.txt")
    try {
      val brCategories = new BufferedReader(new FileReader(categories))
      try {
        var temp = brCategories.readLine
        while (temp != null) {
          val curFileName = if (train) this.dataDirectory + File.separator + "train" + File.separator + temp.split(",")(0) + ".txt"
          else this.dataDirectory + File.separator + "test" + File.separator + temp.split(",")(0) + ".txt"
          val currFile = new File(curFileName)
          val currBR = new BufferedReader(new FileReader(currFile))
          val tempList = new util.ArrayList[String]

          var tempCurrLine = currBR.readLine
          while (tempCurrLine != null) {
            tempList.add(tempCurrLine)
            totalNews += 1
            tempCurrLine = currBR.readLine
          }
          currBR.close()
          val tempPair: Pair[String, util.List[String]] = Pair.of(temp, tempList)
          this.categoryData.add(tempPair)
          temp = brCategories.readLine
        }
        brCategories.close()
      } catch { case e: Exception =>
        println("Exception in reading file :" + e.getMessage)
      } finally {
        if (brCategories != null) brCategories.close()
      }
    }
  }

  def totalExamples: Int =
    this.totalNews

  def inputColumns: Int =
    vectorSize

  def totalOutcomes: Int =
    this.categoryData.size

  def reset(): Unit = {
    _cursor = 0
    newsPosition = 0
    currCategory = 0
  }

  def resetSupported: Boolean =
    true

  def asyncSupported: Boolean =
    true

  def batch: Int =
    batchSize

  def cursor: Int =
    _cursor

  def numExamples: Int =
    totalExamples

  def setPreProcessor(preProcessor: DataSetPreProcessor) {
    throw new UnsupportedOperationException
  }

  def getLabels: util.List[String] =
    this.labels

  def hasNext: Boolean =
    _cursor < numExamples

  def next: DataSet =
    next(batchSize)

  override def remove(): Unit = { }

  def getPreProcessor: DataSetPreProcessor =
    throw new UnsupportedOperationException("Not implemented")

  def getMaxLength: Int =
    this.maxLength

}
